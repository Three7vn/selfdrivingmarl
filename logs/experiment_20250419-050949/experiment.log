2025-04-19 05:09:49,697 - marl_agent - INFO - Starting experiment with config: {'environment': {'env_id': 'intersection-v0', 'render_mode': 'human', 'dt': 0.1, 'max_episode_steps': 250, 'observation_type': 'Kinematics', 'vehicles_count': 15, 'collision_reward': -10.0, 'high_speed_reward': 0.3, 'normalize_reward': True, 'offroad_terminal': True}, 'agents': {'num_agents': 2, 'observation_space': {'features': True, 'vehicles_count': 10}, 'action_space': {'discrete': True, 'actions': ['LANE_LEFT', 'IDLE', 'LANE_RIGHT', 'FASTER', 'SLOWER']}}, 'training': {'algorithm': 'VPG', 'learning_rate': 0.0003, 'gamma': 0.99, 'epochs': 500, 'episodes_per_epoch': 10, 'batch_size': 64, 'epsilon_start': 1.0, 'epsilon_end': 0.05, 'epsilon_decay': 0.995}, 'network': {'hidden_layers': [256, 128], 'activation': 'relu'}, 'logging': {'log_interval': 10, 'checkpoint_interval': 50, 'tensorboard': True}}
2025-04-19 05:09:53,071 - marl_agent - INFO - Epoch 0, Episode 0: Rewards=[3.809090909090909, 3.428181818181818]
2025-04-19 05:09:59,648 - marl_agent - INFO - Epoch 0, Episode 9: Rewards=[1.9000000000000001, 1.7100000000000002]
2025-04-19 05:09:59,651 - marl_agent - INFO - Epoch 0: Mean Rewards=[2.476363636363636, 2.228727272727273], Mean Loss=0.0
2025-04-19 05:10:01,473 - marl_agent - INFO - Epoch 1, Episode 0: Rewards=[3.809090909090909, 3.428181818181818]
